{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fd38434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:05.629808Z",
     "start_time": "2022-11-25T11:56:04.062546Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn import datasets\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, layers, losses, optimizers\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d6d40",
   "metadata": {},
   "source": [
    "**Differential Privacy**\n",
    "\n",
    "The goal of differential privacy is to minimize the effect, a single training sample can have of the output of a (randomized) algorithm. Therefore, perform the following tasks:\n",
    "1) Load the boston housing dataset, one-hot encode the categorical features and normalize (use StandardScaler) the continuous features.\n",
    "\n",
    "2) Split the data into train/test (80% / 20%) sets (give a seed for reproducibility, i.e. random_state=42).\n",
    "\n",
    "3) Train a LinearRegression model (set fit_intercept=False) on the training set and compute r2_score and mean_squared_error on the test set.\n",
    "\n",
    "4) Find the training sample with the largest prediction error. Create a mask to exclude that **sample** from the training set.\n",
    "\n",
    "5) Fit the same LinearRegression model to the training set excluding the found **sample**. Measure the deviation of the two regression models (e.g. np.linalg.norm(w-w'), the weights w can be found in LinearRegression.coef_)\n",
    "\n",
    "6) Add noise of varying scales to the training set, create a second training set by excluding the **sample** and train linear regression models on each. How does the utility (r2_score, mse) behave with increasing noise level? What happens to the difference between the fitted models' weights? What about the prediction error on the **sample**? Plot the r2_score, mse, prediction error on **sample** and weight difference over the noise scale.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56fa94ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:05.649210Z",
     "start_time": "2022-11-25T11:56:05.631370Z"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "boston_dataset = datasets.load_boston()\n",
    "X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "Y = pd.DataFrame(boston_dataset.target, columns=['price'])\n",
    "\n",
    "# normalize continuous features\n",
    "cont_features = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "feature_scaler = StandardScaler()\n",
    "X_pp = X.copy()\n",
    "X_pp[cont_features] = feature_scaler.fit_transform(X[cont_features])\n",
    "X_pp.head()\n",
    "\n",
    "# normalize targets\n",
    "Y_pp = Y.copy()\n",
    "target_scaler = StandardScaler()\n",
    "Y_pp[['price']] = target_scaler.fit_transform(Y[['price']])\n",
    "\n",
    "# one-hot-encode the 'RAD' feature\n",
    "X_pp[['CHAS', 'RAD']] = X_pp[['CHAS', 'RAD']].astype('int32')\n",
    "X_pp = pd.get_dummies(X_pp, columns=['RAD'])\n",
    "X_pp.head()\n",
    "\n",
    "# split data and target DataFrames into data train, data test, target train and target test datasets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_pp.to_numpy(), Y_pp.to_numpy(), test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf1b12e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:05.652799Z",
     "start_time": "2022-11-25T11:56:05.650507Z"
    }
   },
   "outputs": [],
   "source": [
    "# train regression model and find sample with largest prediction error\n",
    "LR = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# create neighboring dataset, i.e. training set without the sample with largest prediction error\n",
    "\n",
    "# train on that dataset the same model, measure the difference between both models' coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3180e82e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:05.713106Z",
     "start_time": "2022-11-25T11:56:05.654221Z"
    }
   },
   "outputs": [],
   "source": [
    "# train on varying noise scales, you should repeat the computation for each sigma several times and average\n",
    "sigmas = np.logspace(-3,2, 24) # noise scales\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "770f85cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:05.722945Z",
     "start_time": "2022-11-25T11:56:05.716042Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d4148",
   "metadata": {},
   "source": [
    "**Autoencoder**\n",
    "\n",
    "An autoencoder is a model that maps samples into a so-called latent space and then back to the original space. It consists of an encoder model X->Z and a decoder model Z->X. \n",
    "It is used for dimensionality reduction, representation learning and as generative model. The training objective of an autoencoder is usually a combination of reconstruction error and some regularization (on its weights and/or the latent representation).\n",
    "\n",
    "1) Load the mnist dataset, split into 10k training samples and 1000 test samples. Take 1000 training samples for evaluation.\n",
    "\n",
    "2) Build a linear autoencoder (encoder/decoder are linear models each). Train your autoencoder with 2D latent space Z. \n",
    "\n",
    "3) Visualize the results as follows: For 1000 training and 1000 test samples\n",
    "\n",
    "    a) Create a scatterplot of the latent embeddings\n",
    "    \n",
    "    b) Plot the reconstructions of 100 samples in a 10x10 grid\n",
    "    \n",
    "    c) Plot the corresponding original samples below\n",
    "    \n",
    "3) Build a non-linear autoencoder. The encoder has 2 conv layers (32/64 filters of size (3,3), strides 2), followed by a dense layer that maps to 2-dimensional latent variables. The decoder consists of a dense layer, followed by 2 conv-transpose layers (64/32, ... matching the encoder). Train the non-linear autoencoder and visualize the results as above.\n",
    "\n",
    "4) Create synthetic samples by randomly sampling 100 points in latent space and decoding them using the linear and non-linear decoders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdbff78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:06.102012Z",
     "start_time": "2022-11-25T11:56:05.725194Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X = x_train.astype('float32').reshape((-1, 28,28,1))/255.\n",
    "Xtrain, Xtest_, Ytrain, Ytest_ = train_test_split(X, y_train, train_size=10000)\n",
    "Xtest, Xtest_, Ytest, Ytest_ = train_test_split(Xtest_, Ytest_, train_size=1000)\n",
    "\n",
    "Xtrain_, _, Ytrain_, _ = train_test_split(Xtrain, Ytrain, train_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ea8ac3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:06.107607Z",
     "start_time": "2022-11-25T11:56:06.104099Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper function to plot several samples in one image\n",
    "def make_grid(X, grid_size=[10,10]):\n",
    "    sh = [X.shape[1], X.shape[2]]\n",
    "    G = np.zeros((sh[0]*grid_size[0], sh[1]*grid_size[1]))\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            G[i*sh[0]:(i+1)*sh[0], j*sh[1]:(j+1)*sh[1]] = X[i*grid_size[1]+j].reshape(sh)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf3ddb76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:06.115361Z",
     "start_time": "2022-11-25T11:56:06.108758Z"
    }
   },
   "outputs": [],
   "source": [
    "# define autoencoder model\n",
    "class AutoEncoder(Model):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(AutoEncoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def call(self, X, training=None):\n",
    "        z = self.encoder(X)\n",
    "        x = self.decoder(z)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "450dfa50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:06.121913Z",
     "start_time": "2022-11-25T11:56:06.116527Z"
    }
   },
   "outputs": [],
   "source": [
    "# linear models    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b141e1a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:06.128467Z",
     "start_time": "2022-11-25T11:56:06.123935Z"
    }
   },
   "outputs": [],
   "source": [
    "# train linear autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15b2a82c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:06.135158Z",
     "start_time": "2022-11-25T11:56:06.129959Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize latent space, reconstructions and original samples for train and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "403930e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:06.216534Z",
     "start_time": "2022-11-25T11:56:06.136607Z"
    }
   },
   "outputs": [],
   "source": [
    "# nonlinear models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c184e298",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:06.283995Z",
     "start_time": "2022-11-25T11:56:06.218964Z"
    }
   },
   "outputs": [],
   "source": [
    "# train non-linear autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4946b37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:06.362086Z",
     "start_time": "2022-11-25T11:56:06.286769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization as before for non-linear model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cf71311",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:06.438834Z",
     "start_time": "2022-11-25T11:56:06.364887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Try out linear decoder as generative model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d63f7ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T11:56:06.503746Z",
     "start_time": "2022-11-25T11:56:06.441746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Non-linear decoder as generative model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
