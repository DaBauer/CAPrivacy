{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction \n",
        "The [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset consits of 60,000 color images of size 32 x 32 collected by by Krizhevsky, Nair, and Hinton. The dataset has 10 classes, each class having 6,000 images which is divided in to two groups: \n",
        "* Training: 50,000 images \n",
        "* Testing: 10,000 images \n",
        "\n",
        "\n",
        "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks."
      ],
      "metadata": {
        "id": "D4he0TjR9g_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following, you will apply different algorithms to the task of object recognition in the images of the aforementioned dataset. Above each cell is a text file with a task for you to complete. If you do not own a laptop with a capable GPU, you can use [google colab](https://colab.research.google.com). Click File -> Upload Notebook to upload your exercise. Then on the right side of the screen you will find Connect -> View Resources -> Change Runtime Type. Here you should have three options to choose from: None, GPU, and TPU. Select GPU as your runtime type. Note that in the non-Pro version, you can only use GPU for a limited time. However, the exercise was performed with the free version without any problems or issues, so you do not need to upgrade to the Pro version to complete the exercise.   \n",
        "\n"
      ],
      "metadata": {
        "id": "M679NQQ9wBXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing required packages"
      ],
      "metadata": {
        "id": "9makeCqOzDGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn import metrics\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model, layers, losses, callbacks, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "jmWInlVU5Mx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Load the data, rescale it and split the **original training** data into training (10000 samples), validation (1000 samples ) and test set (1000 samples)\n",
        "* Print the shape of your training set\n",
        "* Print the number of samples in each class\n",
        "* Compute the mean and standard deviation of your training and test set"
      ],
      "metadata": {
        "id": "Y38EbH9BAloD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for loading the data \n",
        "\n",
        "# Classes (are given)\n",
        "classes = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
        "\n",
        "# Code for printing the shape of your training set \n",
        "\n",
        "# Code for rescaling the images\n",
        "\n",
        "# Code for splitting the data into train/val/test set \n",
        "\n",
        "# Code for printing the # of samples in each class \n",
        "\n",
        "# Code for computing the mean and std of the training and test set"
      ],
      "metadata": {
        "id": "sdMoqnSdy28O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot a 10 x 10 grid with some random data points, each row representing one of the 10 classes with 10 images from that class."
      ],
      "metadata": {
        "id": "8c2qLn_j2g-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for plotting "
      ],
      "metadata": {
        "id": "4NShmK5b5iRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the convnet from first notebook (Convolutional Neural Network in tensorflow using keras API cell) and train on cifar-10 for **100** epochs. \n",
        "* Create a checkpoint to save the best model (see [ModelCheckpoint](https://keras.io/api/callbacks/model_checkpoint/))\n",
        "* Use the trained model to make predictions on the test data\n",
        "* Print the **classification_report**\n",
        "* Print the **confusion_matrix**\n",
        "* Plot training loss vs. validation loss\n",
        "* Plot training accuracy vs. validation accuracy \n"
      ],
      "metadata": {
        "id": "xFTO3RM2I6fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for training on Cifar-10"
      ],
      "metadata": {
        "id": "6OLoVe52McY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for prediction "
      ],
      "metadata": {
        "id": "re3GCl7CMgu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for classification report and confusion matric"
      ],
      "metadata": {
        "id": "afFB_xrIMkWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for training loss vs. val loss"
      ],
      "metadata": {
        "id": "GdSmvuW5rcaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for training acc vs. val acc"
      ],
      "metadata": {
        "id": "Ao8G0ODGrgFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use your trained convnet as a feature extractor and feed the output of the flatten-layer into a **Support Vector Machine**, which now acts as a classification head (fit an SVC to your training data). Then use the fitted SVC to make predictions on your test data. Print its classification report and the confusion matrix. "
      ],
      "metadata": {
        "id": "McmtohXxNkGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for the conv feature extractor + SCV classifier"
      ],
      "metadata": {
        "id": "y284lb5uQtTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for making predictions"
      ],
      "metadata": {
        "id": "nZW93uT-RoKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for printing classification report and confusion matrix"
      ],
      "metadata": {
        "id": "DFMH_8P9Rrmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a NN with the following structure: \n",
        "* Input layer -> Conv block 0 -> Conv block 1 -> Conv block 2 -> Conv block 3 -> Dense layer -> Output layer \n",
        "* Conv block 0: Conv2D(32, (3,3)) -> Conv2D(32, (3,3))\n",
        "* Conv block 1: Conv2D(64, (3,3)) -> Conv2D(32, (3,3))\n",
        "* Conv block 2: Conv2D(128, (3,3)) -> Conv2D(32, (3,3))\n",
        "* Conv block 3: Conv2D(128, (3,3)) -> Conv2D(32, (3,3))\n",
        "* Padding is always '**same**', kernel_initializer is '**he_uniform**', and activation function is '**relu**' \n",
        "* Dense layer: layers.Flatten() -> layers.Dense(512, activation='relu')\n",
        "* Output layer: layers.Dense(10) \n",
        "\n",
        "* Train this model on the cifar-10 data for 100 epochs and make predictions on the test data\n",
        "* Create a checkpoint to save the best model (see [ModelCheckpoint](https://keras.io/api/callbacks/model_checkpoint/))\n",
        "* Print the classification report and confusion matrix\n",
        "* Plot training loss vs. validation loss\n",
        "* Plot Training accuracy vs. validation accuracy \n",
        "* Did the results improve in comparison to the last model? "
      ],
      "metadata": {
        "id": "MflA3SKgV6Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for model 2 "
      ],
      "metadata": {
        "id": "cm8_M5YDRyYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the above model, add a batch normalization layer **after each** Conv2D layer. \n",
        "* Batch normalization: layers.BatchNormalization()\n",
        "* You should have two batch normalization layers in each Conv block\n",
        "* Train this model on the cifar-10 data for 100 epochs and make predictions on the test data\n",
        "* Create a checkpoint to save the best model (see [ModelCheckpoint](https://keras.io/api/callbacks/model_checkpoint/))\n",
        "* Print the classification report and confusion matrix\n",
        "* Plot training loss vs. validation loss\n",
        "* Plot training accuracy vs. validation accuracy \n",
        "* Did the results improve in comparison to the previous two models?\n"
      ],
      "metadata": {
        "id": "VUsqOdoZZYGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for model 3"
      ],
      "metadata": {
        "id": "XyZG8jbccT75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the following changes to the previous model: \n",
        "* In Conv block 0: add a Dropout layer with a rate of 0.1 after the second batch normalization layer \n",
        "* In Conv block 1: add a Dropout layer with a rate of 0.2 after the second batch normalization layer\n",
        "* In Conv block 2: add a Dropout layer with a rate of 0.3 after the second batch normalization layer\n",
        "* In Conv block 3: add a Dropout layer with a rate of 0.4 after the second batch normalization layer\n",
        "* Add a Dropout layer after the Dense layer with rate 0.5 \n",
        "* Dropout layer: layers.Dropout(rate)\n",
        "* Train this model on the cifar-10 data for 100 epochs and make predictions on the test data\n",
        "* Create a checkpoint to save the best model (see [ModelCheckpoint](https://keras.io/api/callbacks/model_checkpoint/))\n",
        "* Print the classification report and confusion matrix\n",
        "* Plot training loss vs. validation loss\n",
        "* Plot training accuracy vs. validation accuracy \n",
        "* Did the results improve in comparison to the previous models?\n",
        "\n"
      ],
      "metadata": {
        "id": "nOnNGRV7aS35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for model 4 "
      ],
      "metadata": {
        "id": "G2PJl82ecWx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply [image augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation) to the previous model. You can use random translation, random flip, and random rotation. \n",
        "* Train this model on the cifar-10 data for 200 epochs and make predictions on the test data\n",
        "* Create a checkpoint to save the best model (see [ModelCheckpoint](https://keras.io/api/callbacks/model_checkpoint/))\n",
        "* Print the classification report and confusion matrix\n",
        "* Plot training loss vs. validation loss\n",
        "* Plot training accuracy vs. validation accuracy \n",
        "* Did the results improve in comparison to the previous models?"
      ],
      "metadata": {
        "id": "CuodsjM9xMmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for image augmentation + the rest of the tasks mentioned above"
      ],
      "metadata": {
        "id": "QAgjzatO0HF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Create 3D scatter plots (per class) of the first 3 principal components on:\n",
        " * Raw images \n",
        " * Simple Convnet features\n",
        " * Features of the best performing model \n",
        "\n",
        " Describe your observation from the three plots\n"
      ],
      "metadata": {
        "id": "jIQBNxWQcblA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for the three different 3D scatter plots "
      ],
      "metadata": {
        "id": "KDPaxRfNrI4e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}